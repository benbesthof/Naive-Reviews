{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this model will be to test which form of word reduction, stemming or lemmatization, leads to the best performing Naive Bayes classifier for movie reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: regex in /usr/local/python/3.10.13/lib/python3.10/site-packages (2023.12.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk \n",
    "!pip install regex\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import regex as re \n",
    "import sklearn \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with unstructured data, specifically text, can get tedious because it takes many forms like reviews or articles. All these forms need to be stored as documents that then need to be compiled into a corpus. In this case each document (review) is stored in a text file and a function is needed to extract the reviews from the text file and land it into a pandas dataframe. Populating a dataset with the reviews will make the corpus of text more readable and easier to analyze. Therefore, a pandas dataframe is created with two variables:\n",
    "\n",
    "-Sentiment: the class label of the review\n",
    "\n",
    "-Review: textual content of movie review\n",
    "\n",
    "The unit of analysis is the sentiment expressed by the movie review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my inner flag was at half-mast last year when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\" when you get out of jail , you can kill him...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>playwright tom stoppard and screenwriter marc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>capsule : trippy , hyperspeed action machine f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>the most absurd remake of 1998 ? \\nit's a toss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          0  my inner flag was at half-mast last year when ...\n",
       "1          1   \" when you get out of jail , you can kill him...\n",
       "2          1  playwright tom stoppard and screenwriter marc ...\n",
       "3          1  capsule : trippy , hyperspeed action machine f...\n",
       "4          0  the most absurd remake of 1998 ? \\nit's a toss..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_path = '/workspaces/Naive-Reviews/neg/'\n",
    "pos_path = '/workspaces/Naive-Reviews/pos/'\n",
    "\n",
    "def get_docs(path, label):\n",
    "\n",
    "    \"\"\" Extractor of text from text files stored in a directory. Populator of existing pandas dataframe wih extracted text data\n",
    "\n",
    "        Args:\n",
    "            path (str): path to directory containing text files\n",
    "            \n",
    "            label (int): class label of document\n",
    "        Returns:\n",
    "            dataframe (pd.DataFrame): dataframe with extracted text and class labels\n",
    "    \"\"\"\n",
    "    #out of this list we will create a dataframe\n",
    "    data = []\n",
    "    #open directory\n",
    "    folder = os.listdir(path)\n",
    "    for i in folder:\n",
    "        files = os.path.join(path, i)\n",
    "        #extract text from each file in directory and append the values to the dataframe\n",
    "        with open(files, 'r') as file:\n",
    "            review_text = file.read()\n",
    "            #add the value pairs review text and sentiment as dictionaries to list so that dictionary can be made into dataframe\n",
    "            data.append({'sentiment': label,'review': review_text})\n",
    "    #convert list to dataframe. Pandas can interpret the column row relationship based of key value pairs\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "df_neg = get_docs(neg_path, 0)\n",
    "df_pos = get_docs(pos_path, 1)\n",
    "#need to union the data together \n",
    "\n",
    "#shuffle data so that the model does not interpret a pattern based on position of sentiments (all the negative reviews being listed first and the postive ones last)\n",
    "df = pd.concat([df_neg, df_pos], axis = 0).sample(frac = 1).reset_index(drop = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are punctiation marks and in the fifth row there is a newline character ('\\n') for every time the sentence continues on to the next line of the text file. All of the punctuation marks and newline characters have to be removed from the text so that they are not intepreted as a letter or an individual word during modeling. Stop words like 'to' and 'and' need to be removed because they tend to have higher term frequencies than other words. Additonally all the words will be turned to lower case so that a single word will not be interpreted twice.\n",
    "\n",
    "Cleaning the data will also require for lemmatization or stemming of the text. Lemmatization and stemming are done using nltk and their goal is to reduce the frequency of words with the same meaning. \n",
    "\n",
    "**Stemming**\n",
    "\n",
    "-Reduce the word to its root form. For example, 'bought' and 'buying' are reduced to 'buy'\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "-Reduces the word to its definitional form. For example, 'am' and 'is' are reduced to 'be'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Na√Øve Bayes Classifier Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
